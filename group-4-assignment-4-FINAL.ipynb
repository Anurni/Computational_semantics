{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4: LSTMs and Transformers for Word Sense Disambiguation\n",
    "\n",
    "by Nikolai Ilinykh, Adam Ek, and others.\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "\n",
    "A problem with static distributional vectors is the difficulty of distinguishing between different *word senses*. We will continue our exploration of word vectors by considering *trainable vectors* or *word embeddings* for Word Sense Disambiguation (WSD). We will work with both LSTMs and transformer models, e.g. BERT. The purpose of the assignment is to learn use representations neural models in a downstream task of word sense disambiguation.\n",
    "\n",
    "\n",
    "## Word Sense Disambiguation Task\n",
    "\n",
    "The goal of word sense disambiguation is to train a model to find the sense of a word (homonyms of a word-form). For example, the word \"bank\" can mean \"sloping land\" or \"financial institution\". \n",
    "\n",
    "(a) \"I deposited my money in the **bank**\" (financial institution)\n",
    "\n",
    "(b) \"I swam from the river **bank**\" (sloping land)\n",
    "\n",
    "In case a) and b), we can determine the meaning of \"bank\" based on the *context*. To utilize context in a semantic model, we use *contextualized word representations*.\n",
    "\n",
    "Previously, we worked with *static word representations*, i.e., the representation does not depend on the context. To illustrate, we can consider sentences (a) and (b), where the word **bank** would have the same static representation in both sentences, which means that it becomes difficult for us to predict its sense. What we want is to create representations that depend on the context, i.e., *contextualized embeddings*.\n",
    "\n",
    "As we have discussed in the class, contextualized representations can come in the form of pre-training the model for some \"general\" task and then fine-tuning it for some downstream task. Here we will do the following:\n",
    "\n",
    "(1) Train and test LSTM model directly for word sense disambiguation. We will learn contextualized representations within this model.\n",
    "\n",
    "(2) Take BERT that was pre-trained on masked language modeling and next sentence prediction. Fine-tune it on our data and test it for the word sense disambiguation on the task dataset. The idea for you is to explore how pre-trained contextualized representations from BERT can be updated and used for the downstream task of word sense disambiguation.\n",
    "\n",
    "Your overall task in this lab is to create a neural network model that can disambiguate the word sense of 30 different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we import some packages that we need\n",
    "\n",
    "# here add any package that you will need later\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torchtext\n",
    "import pandas as pd\n",
    "\n",
    "# our hyperparameters (add more when/if you need them)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.000001\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Working with Data\n",
    "\n",
    "A central part of any machine learning system is the data we're working with.\n",
    "\n",
    "In this section, we will split the data (the dataset is in `wsd_data.txt`) into a training set and a test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset we will use contains different word senses for 30 different words. The data is organized as follows (values separated by tabs), where each line is a separate item in the dataset:\n",
    "\n",
    "- Column 1: word-sense, e.g., keep%2:42:07::\n",
    "- Column 2: word-form, e.g., keep.v\n",
    "- Column 3: index of word, e.g., 15\n",
    "- Column 4: white-space tokenized context, e.g., Action by the Committee In pursuance of its mandate , the Committee will continue to keep under review the situation relating to the question of Palestine and participate in relevant meetings of the General Assembly and the Security Council . The Committee will also continue to monitor the situation on the ground and draw the attention of the international community to urgent developments in the Occupied Palestinian Territory , including East Jerusalem , requiring international action .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "\n",
    "Your first task is to separate the data into a *training set* and a *test set*.\n",
    "\n",
    "The training set should contain 80% of the examples, and the test set the remaining 20%.\n",
    "\n",
    "The examples for the test/training set should be selected **randomly**.\n",
    "\n",
    "Save each dataset into a .csv file for loading later.\n",
    "\n",
    "**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import csv\n",
    "\n",
    "def data_split(dataset_path):\n",
    "    with open (dataset_path) as file:\n",
    "        data = file.readlines()\n",
    "    train_split, test_split = train_test_split(data,test_size=0.2,random_state=42)\n",
    "    #print(train_split[:20])\n",
    "\n",
    "    with open('train_data.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, quotechar=\" \", quoting=csv.QUOTE_MINIMAL)\n",
    "        for row in train_split:\n",
    "            writer.writerow([row])\n",
    "    with open('test_data.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, quotechar=\" \",quoting=csv.QUOTE_MINIMAL)\n",
    "        for row in test_split:\n",
    "            writer.writerow([row])\n",
    "\n",
    "    return train_split,test_split\n",
    "\n",
    "train_data,test_data = data_split('./wsd_data.txt')    #REMEMBER TO CHANGE BACK!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Baseline\n",
    "\n",
    "Your second task is to create a *baseline* for the task.\n",
    "\n",
    "A baseline is a \"reality check\" for a model. Given a very simple heuristic/algorithmic/model solution to the problem, can our neural network perform better than this? Baselines are important as they give us a point of comparison for the actual models. They are commonly used in NLP. Sometimes baseline models are not simple models but previous state-of-the-art.\n",
    "\n",
    "In this exercise, we will have a simple baseline model that is the \"most common sense\" (MCS) baseline. For each word form, find the most commonly assigned sense to the word and label a word with that sense. In a fictional dataset, \"bank\" has two senses: \"financial institution,\" which occurs 5 times, and \"side of the river,\" which occurs 3 times. Thus, all 8 occurrences of \"bank\" are labeled \"financial institution,\" yielding an MCS accuracy of 5/8 = 62.5%. If a model obtains a higher score than this, we can conclude that the model *at least* is better than selecting the most frequent word sense.\n",
    "\n",
    "Your task is to write the code for this baseline, train, and test it. The baseline has the knowledge about labels and their frequency only from the train data. You evaluate it on the test data by comparing the ground-truth sense with the one that the model predicts. A good \"dumb\" baseline in this case is the one that performs quite badly. Expect the model to perform around 0.30 in terms of accuracy. You should use accuracy as your main metric; you can also compute the F1-score.\n",
    "\n",
    "**[2 marks]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_907865/1483950464.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  train = pd.read_csv('train_data.csv',delimiter='\\\\t',header=None,names=[\"word_sense\", \"word_form\",\"index\", \"context\"])\n",
      "/tmp/ipykernel_907865/1483950464.py:31: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  test = pd.read_csv('test_data.csv',delimiter='\\\\t',header=None,names=[\"word_sense\", \"word_form\",\"index\", \"context\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.30723208415516107"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def mcs_baseline(data):\n",
    "    wordform_dict = {}\n",
    "    final_dict = {}\n",
    "    train = pd.read_csv('train_data.csv',delimiter='\\\\t',header=None,names=[\"word_sense\", \"word_form\",\"index\", \"context\"])\n",
    "    for index, row in train.iterrows():\n",
    "        if row[\"word_sense\"] not in wordform_dict.keys():\n",
    "            wordform_dict[row[\"word_sense\"]] = 0\n",
    "        wordform_dict[row[\"word_sense\"]] += 1\n",
    "\n",
    "    for index, row in train.iterrows():\n",
    "        senses_list = []\n",
    "        for word_sense,frequency in wordform_dict.items():\n",
    "            if word_sense[:3] in row[\"word_form\"]:\n",
    "                senses_list.append((word_sense,frequency))\n",
    "        final_dict[row[\"word_form\"]] = senses_list\n",
    "\n",
    "    for word_form,word_sense in final_dict.items():\n",
    "        most_common_sense = 0\n",
    "        most_common_word = \" \"\n",
    "        for most_common in word_sense:\n",
    "            if most_common[1] > most_common_sense:\n",
    "                most_common_sense = most_common[1]\n",
    "                most_common_word = most_common[0]\n",
    "        final_dict[word_form] = most_common_word\n",
    "\n",
    "    #getting the accuracy with always predicting the most common label \n",
    "\n",
    "    test = pd.read_csv('test_data.csv',delimiter='\\\\t',header=None,names=[\"word_sense\", \"word_form\",\"index\", \"context\"])\n",
    "    corrects = 0\n",
    "    for index, row in test.iterrows():\n",
    "        if row[\"word_sense\"] in final_dict.values():\n",
    "            corrects+=1\n",
    "    return corrects/len(test[\"word_form\"]) \n",
    "\n",
    "mcs_baseline(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Iterators\n",
    "\n",
    "To train a neural network, we first need to prepare the data. This involves converting words (and labels) to a number and organizing the data into batches. We also want the ability to shuffle the examples such that they appear in a random order.\n",
    "\n",
    "Your task is to create a dataloader for the training and test set you created previously.\n",
    "\n",
    "You are encouraged to adjust your own dataloader you built for previous assignments. Some things to take into account:\n",
    "\n",
    "1. Tokenize inputs, keep a dictionary of word-to-IDs and IDs-to-words (vocabulary), fix paddings. You might need to consider doing these for each of the four fields in the dataset.\n",
    "2. Your dataloader probably has a function to process data. Process each column in the dataset.\n",
    "3. You might want to clean the data a bit. For example, the first column has some symbols, which might be unnecessary. It is up to you whether you want to remove them and clean this column or keep labels the way they are. In any case, you must provide an explanation of your decision and how you think it will affect the performance of your model. Data and its preprocessing matters, so motivate your decisions.\n",
    "4. Organize your dataset into batches and shuffle them. You should have something akin to data iterators so that your model can take them.\n",
    "\n",
    "Implement the dataloader and perform necessary preprocessings.\n",
    "\n",
    "[**2 marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class WSD_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file):\n",
    "        \n",
    "        self.file = pd.read_csv(csv_file, delimiter='\\\\t',header=None,names=[\"word_sense\", \"word_form\",\"index\", \"context\"])\n",
    "\n",
    "        self.senses = [sense.replace(\":\", \"\").replace(\"%\", \"\") for sense in self.file['word_sense']]\n",
    "        self.words = [line for line in self.file['word_form']]\n",
    "        self.indices = [int(x) for x in self.file['index']]\n",
    "        self.contexts = [line.lower().split() for line in self.file['context']]\n",
    "\n",
    "\n",
    "        # vocab for senses\n",
    "        self.sense_vocab_s2i = {s: num for num, s in enumerate(list(set(self.senses)))}\n",
    "        self.sense_vocab_i2s = {num: s for num, s in enumerate(list(set(self.senses)))}\n",
    "        \n",
    "        \n",
    "        # padding\n",
    "        self.context_lengths = []\n",
    "        for c in self.contexts:\n",
    "            self.context_lengths.append(len(c))\n",
    "        pad_number = max(self.context_lengths)\n",
    "        \n",
    "        self.padded_contexts = []\n",
    "        for c in tqdm(self.contexts):\n",
    "            while len(c) < pad_number:\n",
    "                c.append('<pad>')\n",
    "            self.padded_contexts.append(c)\n",
    "\n",
    "        # create vocabulary for contexts\n",
    "        self.tokens = list(set([token for line in self.padded_contexts for token in line]))\n",
    "        # vocab for context\n",
    "        self.context_w2i = {t: num for num, t in enumerate(self.tokens)}\n",
    "        self.context_i2w = {num: t for num, t in enumerate(self.tokens)}\n",
    "        \n",
    "        # encode input contexts\n",
    "\n",
    "        self.int_contexts = []\n",
    "        for c in self.padded_contexts:\n",
    "            encoded_c = []\n",
    "            for w in c:\n",
    "                encoded_c.append(self.context_w2i[w])\n",
    "            self.int_contexts.append(encoded_c)\n",
    "\n",
    "        self.int_senses = []\n",
    "        for s in self.senses:\n",
    "            self.int_senses.append([self.sense_vocab_s2i[s]])\n",
    "            \n",
    "            \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #print(self.int_contexts[idx])\n",
    "        \n",
    "        return ([self.int_contexts[idx]], self.int_senses[idx], self.indices[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.int_senses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_918301/723441646.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  self.file = pd.read_csv(csv_file, delimiter='\\\\t',header=None,names=[\"word_sense\", \"word_form\",\"index\", \"context\"])\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 60839/60839 [00:00<00:00, 79214.39it/s]\n",
      "/tmp/ipykernel_918301/723441646.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  self.file = pd.read_csv(csv_file, delimiter='\\\\t',header=None,names=[\"word_sense\", \"word_form\",\"index\", \"context\"])\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15210/15210 [00:00<00:00, 82335.83it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = WSD_Dataset('./train_data.csv')\n",
    "test_dataset = WSD_Dataset('./test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60839"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.int_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70237"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 LSTM for Word Sense Disambiguation\n",
    "\n",
    "In this section, we will train an LSTM model to predict word senses based on *contextualized representations*.\n",
    "\n",
    "You can read more about LSTMs [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We will use a **bidirectional** Long Short-Term Memory (LSTM) network to create a representation for the sentences and a **linear** classifier to predict the sense of each word.\n",
    "\n",
    "As we discussed in the lecture, bidirectional LSTM is using **two** hidden states: one that goes in the left-to-right direction, and another one that goes in the right-to-left direction. PyTorch documentation on LSTMs can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). It says that if the bidirectional parameter is set to True, then \"h_n will contain a concatenation of the final forward and reverse hidden states, respectively.\" Keep it in mind because you will have to ensure that your linear layer for prediction takes input of that size.\n",
    "\n",
    "When we initialize the model, we need a few things:\n",
    "\n",
    "1) An embedding layer: a dictionary from which we can obtain word embeddings\n",
    "2) A LSTM-module to obtain contextual representations\n",
    "3) A classifier that computes scores for each word-sense given *some* input\n",
    "\n",
    "The general procedure is the following:\n",
    "\n",
    "1) For each word in the sentence, obtain word embeddings\n",
    "2) Run the embedded sentences through the LSTM\n",
    "3) Select the appropriate hidden state\n",
    "4) Predict the word-sense \n",
    "\n",
    "**Suggestion for efficiency:** *Use a low dimensionality (32) for word embeddings and the LSTM when developing and testing the code, then scale up when running the full training/tests*\n",
    "\n",
    "Your tasks will be to create **two different models** (both follow the two outlines described above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first model should make a prediction from the LSTM's representation of the target word.\n",
    "\n",
    "In particular, you run your LSTM on the context in which the target word is used. LSTM will produce a sequence of hidden states. Each hidden state corresponds to a single word from the input context. For example, you should be able to get 37 hidden states for a context that has 37 words/elements in it. Next, take the LSTM's representation of the target word. For example, it can be hidden state number 5, because the fifth word in your context is the target word that you want to predict the meaning for. This target's word representation is the input to your linear layer that makes the final prediction.\n",
    "\n",
    "**[5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSDModel_approach1(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size, embedding_dim, vocab_sense_size): #hidden size 512\n",
    "        \n",
    "        super(WSDModel_approach1,self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers=1, bidirectional=True) #, batch_first=True)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=hidden_size*2, out_features=vocab_sense_size)\n",
    "        \n",
    "    def forward(self, input_seqs):\n",
    "        # Translate batches into embeddings\n",
    "        \n",
    "        embedded_context = self.embeddings(input_seqs.to(device))\n",
    "                \n",
    "        lstm_out, (h, c) = self.rnn(embedded_context.to(device))\n",
    "        #print(lstm_out.shape)     \n",
    "        target_hidden = torch.zeros((8, 1, 1024)) # batch_size, context, hidden_size\n",
    "        for num, ind in enumerate(indices.to(device)):            \n",
    "            target_hidden[num, :, :] = lstm_out[num, ind, :]    #problem possibly here?\n",
    "        #print(target_hidden.shape)\n",
    "        predictions = self.classifier(target_hidden.to(device))\n",
    "              \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = WSDModel_approach1(len(dataset.tokens), 512, 512, len(dataset.sense_vocab_s2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#print(input_seqs.shape)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m input_seqs \u001b[38;5;241m=\u001b[39m input_seqs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 18\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seqs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#.detach().numpy()\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m,out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m, in \u001b[0;36mWSDModel_approach1.forward\u001b[0;34m(self, input_seqs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_seqs):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Translate batches into embeddings\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     embedded_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     lstm_out, (h, c) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(embedded_context\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#print(lstm_out.shape)     \u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.11/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "#model 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for batch in dataloader:\n",
    "    \n",
    "    input_seqs = [torch.stack(elem) for elem in batch[0]]\n",
    "\n",
    "        \n",
    "    gold_labels = batch[1][0].to(device)\n",
    "    indices = batch[2].long().to(device)\n",
    "    #print(indices)\n",
    "    #print(\"indicesshape\",indices.shape)\n",
    "    input_seqs = input_seqs[0].to(device)\n",
    "    \n",
    "    #print(input_seqs.shape)\n",
    "    input_seqs = input_seqs.permute(1, 0).to(device)\n",
    "    out = model1(input_seqs).squeeze(1)#.detach().numpy()\n",
    "    print('out',out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your second model should make a prediction from the final hidden state of your LSTM.\n",
    "\n",
    "In particular, do the same first steps as in the first approach. But then to make a prediction with your linear layer, you will need to take the last hidden state that your LSTM produces for the whole sequence.\n",
    "\n",
    "**[5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSDModel_approach2(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size, embedding_dim, vocab_sense_size):\n",
    "        \n",
    "        super(WSDModel_approach2,self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=hidden_size*2, out_features=vocab_sense_size)\n",
    "        \n",
    "    def forward(self, input_seqs):\n",
    "        # Translate batches into embeddings\n",
    "        \n",
    "        embedded_context = self.embeddings(input_seqs)\n",
    "                \n",
    "        lstm_out, (h, c) = self.rnn(embedded_context)\n",
    "        #print(\"this is h.shape\", h.shape)\n",
    "        \n",
    "        h = h.permute(1, 0, 2)\n",
    "        h = h.reshape(8, 1024)   \n",
    "        #print(h.shape)\n",
    "\n",
    "        predictions = self.classifier(h)\n",
    "              \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = WSDModel_approach2(len(dataset.tokens), 512, 512, len(dataset.sense_vocab_s2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for batch in dataloader:\n",
    "    \n",
    "    input_seqs = [torch.stack(elem) for elem in batch[0]]\n",
    "\n",
    "        \n",
    "    gold_labels = batch[1][0]\n",
    "    indices = batch[2].long()\n",
    "    input_seqs = input_seqs[0]\n",
    "    #print(len(input_seqs))\n",
    "    #print(indices)\n",
    "    input_seqs = input_seqs.permute(1, 0)\n",
    "    \n",
    "    \n",
    "    out = model2(input_seqs).squeeze(1).detach().numpy()\n",
    "    #print(\"outshape\",out.shape)\n",
    "    \n",
    "    #print(out[0])\n",
    "\n",
    "    max_prediction = np.argmax(out, axis=1)\n",
    "    #print(max_prediction)\n",
    "    \n",
    "    #print(dataset.sense_vocab_i2s[max_prediction[0]])\n",
    "    \n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing the Model\n",
    "\n",
    "Now we are ready to train and test our model. What we need now is a loss function, an optimizer, and our data. \n",
    "\n",
    "- First, create the loss function and the optimizer.\n",
    "- Next, iterate over the number of epochs (i.e., how many times we let the model see our data). \n",
    "- For each epoch, iterate over the dataset to obtain batches. Use the batch as input to the model, and let the model output scores for the different word senses.\n",
    "- For each model output, calculate the loss (and print the loss) on the output and update the model parameters.\n",
    "- Reset the gradients and repeat.\n",
    "- After all epochs are done, test your trained model on the test set and calculate the total and per-word-form accuracy of your model.\n",
    "\n",
    "Implement the training and testing of the model.\n",
    "\n",
    "**[4 marks]**\n",
    "\n",
    "**Suggestion for efficiency:** *When developing your model, try training and testing the model on one or two batches (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*\n",
    "\n",
    "Do not forget to save your best models as .pickle files. The results should be reproducible for us to evaluate your models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to shape mismatch: torch.Size([7]) vs torch.Size([8, 222])\n",
      "Epoch 1, Loss : 1.786867834724618\n",
      "Skipping batch due to shape mismatch: torch.Size([7]) vs torch.Size([8, 222])\n",
      "Epoch 2, Loss : 1.5498475718545255\n",
      "Skipping batch due to shape mismatch: torch.Size([7]) vs torch.Size([8, 222])\n",
      "Epoch 3, Loss : 1.540198791563942\n",
      "Skipping batch due to shape mismatch: torch.Size([7]) vs torch.Size([8, 222])\n",
      "Epoch 4, Loss : 1.5327540545460427\n",
      "Skipping batch due to shape mismatch: torch.Size([7]) vs torch.Size([8, 222])\n",
      "Epoch 5, Loss : 1.5270262987577625\n",
      "Skipping batch due to shape mismatch: torch.Size([8]) vs torch.Size([2])\n",
      "correct: 214\n",
      "accuracy: 0.11251314405888538\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "device = torch.device(\"cuda\")\n",
    "loss_function = CrossEntropyLoss()\n",
    "model1 = model1.to(device)\n",
    "optimizer = optim.AdamW(model1.parameters(),lr=0.00001)\n",
    "\n",
    "#   training loop for the first model\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_seqs = [torch.stack(elem) for elem in batch[0]]  # contexts\n",
    "        gold_labels = batch[1][0].to(device)   # gold label in the batch\n",
    "        indices = batch[2].long().to(device)   # indices of the target words??\n",
    "        input_seqs = input_seqs[0].to(device) # contexts\n",
    "        input_seqs = input_seqs.permute(1, 0).to(device)\n",
    "        outputs = model1(input_seqs)#.squeeze(1).to(device)\n",
    "        outputs = outputs.view(outputs.size(0), -1)\n",
    "        if gold_labels.shape[0] != outputs.shape[0]:\n",
    "            print(f\"Skipping batch due to shape mismatch: {gold_labels.shape} vs {outputs.shape}\") #why is there a mismatch we are using dataloaders\n",
    "            continue\n",
    "        loss = loss_function(outputs.to(device), gold_labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss : {total_loss/len(dataloader)}')\n",
    "\n",
    "\n",
    "# testing the model after all epochs are completed\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader_test:\n",
    "        input_seqs = [torch.stack(elem) for elem in batch[0]]   # contexts\n",
    "        gold_labels = batch[1][0].to(device)  # gold label in the batch\n",
    "        indices = batch[2].to(device).long()  # indices of the target words\n",
    "        input_seqs = input_seqs[0].to(device)  # contexts\n",
    "        input_seqs = input_seqs.permute(1, 0).to(device)\n",
    "        outputs = model1(input_seqs).squeeze(1)    \n",
    "        max_prediction = torch.argmax(outputs, dim=1)#.cpu().numpy()\n",
    "        if max_prediction.size() != gold_labels.size():\n",
    "            print(f\"Skipping batch due to shape mismatch: {max_prediction.size()} vs {gold_labels.size()}\") #also why is there mismatch here\n",
    "            continue\n",
    "        correct+=torch.sum(max_prediction == gold_labels).item()\n",
    "    print(\"correct:\",correct)\n",
    "print(\"accuracy:\", correct/len(dataloader_test))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss : 4.660387977497269\n",
      "Epoch 2, Loss : 4.159341004426192\n",
      "Epoch 3, Loss : 3.5921727566480794\n",
      "Epoch 4, Loss : 2.8516270564466146\n",
      "Epoch 5, Loss : 2.022744782898945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_918301/942876917.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_prediction_tensor = torch.tensor(max_prediction).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 76\n",
      "accuracy: 0.03995793901156677\n"
     ]
    }
   ],
   "source": [
    "loss_function = CrossEntropyLoss()\n",
    "model2 = model2.to(device)\n",
    "optimizer = optim.AdamW(model2.parameters(),lr=0.0001)\n",
    "#print(device)\n",
    "#   training loop for the second model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_seqs = [torch.stack(elem) for elem in batch[0]]   # contexts\n",
    "        gold_labels = batch[1][0].to(device)   # gold label in the batch\n",
    "        #print(\"gold_labels\", gold_labels)\n",
    "        #indices = batch[2].long()   # indices of the target words??\n",
    "        input_seqs = input_seqs[0].to(device)  # contexts\n",
    "        input_seqs = input_seqs.permute(1, 0).to(device)\n",
    "        try:\n",
    "            outputs = model2(input_seqs)\n",
    "            #print(\"outputs:\", outputs)\n",
    "            loss = loss_function(outputs, gold_labels)\n",
    "        #print(\"loss\",loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        except:\n",
    "            continue\n",
    "    print(f'Epoch {epoch + 1}, Loss : {total_loss/len(dataloader)}')\n",
    "    \n",
    "# testing the model after all epochs are completed\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader_test:\n",
    "        input_seqs = [torch.stack(elem) for elem in batch[0]]   # contexts\n",
    "        gold_labels = batch[1][0].to(device)   # gold label in the batch\n",
    "        #print(\"gold:\",dataset.sense_vocab_i2s[gold_labels[0].item()])\n",
    "        indices = batch[2].long().to(device)   # indices of the target words??\n",
    "        input_seqs = input_seqs[0].to(device)  # contexts\n",
    "        input_seqs = input_seqs.permute(1, 0).to(device)\n",
    "        try:\n",
    "            outputs = model2(input_seqs)  \n",
    "            max_prediction_tensor = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        #print(\"prediction\",dataset.sense_vocab_i2s[max_prediction[0].item()])\n",
    "            max_prediction_tensor = torch.tensor(max_prediction).to(device)\n",
    "            correct+=int(torch.eq(max_prediction_tensor,gold_labels).sum().item())\n",
    "        except:\n",
    "            continue\n",
    "    print(\"correct:\",correct)\n",
    "print(\"accuracy:\", correct/len(dataloader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Fine-tuning and Testing BERT for Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the lab, you'll try out the transformer, specifically the BERT model. For this, we'll use the Hugging Face library ([https://huggingface.co/](https://huggingface.co/)).\n",
    "\n",
    "You can find the documentation for the BERT model [here](https://huggingface.co/transformers/model_doc/bert.html) and a general usage guide [here](https://huggingface.co/transformers/v2.9.1/quickstart.html).\n",
    "\n",
    "What we're going to do is *fine-tune* the BERT model, i.e., update the weights of a pre-trained model. That is, we have a model that is pre-trained on masked language modeling and next sentence prediction (kind of basic, general tasks which are useful for a lot of more specific tasks), but now we apply it to word sense disambiguation with the word representations it has learned.\n",
    "\n",
    "We'll use the same data splits for training and testing as before, but this time you will use a different dataloader.\n",
    "\n",
    "Now you create an iterator that collects N sentences (where N is the batch size) then use the BertTokenizer to transform the sentence into integers. For your dataloader, remember to:\n",
    "* Shuffle the data in each batch\n",
    "* Make sure you get a new iterator for each *epoch*\n",
    "* Create a vocabulary of *sense-labels* so you can calculate accuracy \n",
    "\n",
    "We then pass this batch into the BERT model (you must have pre-loaded its weights) and update the weights (fine-tune). The BERT model will encode the sentence, then we send this encoded sentence into a prediction layer and collect what it outputs.\n",
    "\n",
    "As input to the prediction layer, you are free to play with different types of information. For example, the expected way would be to use CLS representation. You can also use other representations and compare them.\n",
    "\n",
    "About the hyperparameters and training:\n",
    "* For BERT, usually a lower learning rate works best, between 0.0001-0.000001.\n",
    "* BERT takes a lot of resources, running it on CPU will take ages, utilize the GPUs :)\n",
    "* Since BERT takes a lot of resources, use a small batch size (4-8)\n",
    "* Computing the BERT representation, make sure you pass the mask\n",
    "\n",
    "**[12 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Bert_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file):\n",
    "        \n",
    "        self.file = pd.read_csv(csv_file, delimiter='\\\\t',header=None,names=[\"word_sense\", \"word_form\",\"index\", \"context\"])\n",
    "\n",
    "        self.senses = [sense.replace(\":\", \"\").replace(\"%\", \"\") for sense in self.file['word_sense']]\n",
    "        self.words = [line for line in self.file['word_form']]\n",
    "        self.indices = [int(x) for x in self.file['index']]\n",
    "        self.contexts = [line for line in self.file['context']]\n",
    "       \n",
    "\n",
    "        # vocab for senses\n",
    "        self.sense_vocab_s2i = {s: num for num, s in enumerate(list(set(self.senses)))}\n",
    "        self.sense_vocab_i2s = {num: s for num, s in enumerate(list(set(self.senses)))}\n",
    "        \n",
    "        self.int_senses = []\n",
    "        for s in self.senses:\n",
    "           self.int_senses.append([self.sense_vocab_s2i[s]])\n",
    "\n",
    "        self.unique_senses = []\n",
    "        for s in self.senses:   \n",
    "            if s not in self.unique_senses:\n",
    "                self.unique_senses.append(s)\n",
    "            \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        context = self.contexts[idx]\n",
    "        word_index = self.indices[idx]\n",
    "        word_sense = self.int_senses[idx]\n",
    "        \n",
    "        return context ,word_index,word_sense\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')    \n",
    "def bert_collate(data):\n",
    "    contexts =[x[0] for x in data]\n",
    "    word_indices= [x[1] for x in data]\n",
    "    senses = [x[2] for x  in data]\n",
    "\n",
    "    seqs = tokenizer.batch_encode_plus(\n",
    "        contexts,\n",
    "        add_special_tokens = True,\n",
    "        padding = 'longest',\n",
    "        return_tensors = 'pt'\n",
    "        )\n",
    "    adjusted_word_indices = []\n",
    "    for idx in word_indices:\n",
    "        adjusted_word_indices.append(idx+1)\n",
    "    return seqs['input_ids'], seqs['attention_mask'], torch.tensor(adjusted_word_indices), torch.tensor(senses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_for_bert(path_to_file, batch_size, tokenizer):\n",
    "    bert_dataset = Bert_Dataset(path_to_file)\n",
    "    bert_dataloader = DataLoader(bert_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            collate_fn=bert_collate)\n",
    "    return bert_dataset, bert_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_918301/2153157368.py:11: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  self.file = pd.read_csv(csv_file, delimiter='\\\\t',header=None,names=[\"word_sense\", \"word_form\",\"index\", \"context\"])\n",
      "/tmp/ipykernel_918301/2153157368.py:11: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  self.file = pd.read_csv(csv_file, delimiter='\\\\t',header=None,names=[\"word_sense\", \"word_form\",\"index\", \"context\"])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_dataset,bert_dataloader = dataloader_for_bert('train_data.csv',batch_size=8,tokenizer=tokenizer)\n",
    "bert_dataset_test,bert_dataloader_test = dataloader_for_bert('test_data.csv',batch_size=8,tokenizer=tokenizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import random\n",
    "\n",
    "#initializing the model with pretrained weights\n",
    "\n",
    "class BERT_WSD(nn.Module):\n",
    "    def __init__(self, no_of_labels):\n",
    "        super(BERT_WSD,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size,no_of_labels)\n",
    "    \n",
    "    def forward(self,input_ids,mask,indices):\n",
    "        outputs = self.bert(input_ids.to(device),mask.to(device))\n",
    "        #So the hidden state size is (batch_size, seq_len, hidden_size) \n",
    "        seq_output = outputs.last_hidden_state.to(device)\n",
    "        batch_size = seq_output.size(0)\n",
    "        indices = indices.view(batch_size) \n",
    "        #print(indices)\n",
    "        target = seq_output[torch.arange(batch_size),indices]\n",
    "        \n",
    "        predictions = self.classifier(target)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_918301/2634342556.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  senses = torch.tensor(senses).view(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss : 4.733702104300123\n",
      "Epoch 2, Loss : 3.405372674335076\n",
      "Epoch 3, Loss : 2.1753532283012995\n",
      "Epoch 4, Loss : 1.5793246047227183\n",
      "Epoch 5, Loss : 1.2809866848762375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_918301/2634342556.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  senses = torch.tensor(senses).view(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 5536\n",
      "0.36397107166337933\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "loss_function = CrossEntropyLoss()\n",
    "model = BERT_WSD(len(bert_dataset.unique_senses)).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(),lr=0.000001)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in bert_dataloader:\n",
    "        input_ids,mask,indices,senses = batch\n",
    "        input_ids.to(device)\n",
    "        mask.to(device)\n",
    "        indices.to(device)\n",
    "        senses = torch.tensor(senses).view(-1)\n",
    "        senses.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "            \n",
    "        outputs = model(input_ids,mask,indices).to(device)\n",
    "        #print(outputs)\n",
    "        loss = loss_function(outputs.to(device),senses.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss : {total_loss/len(dataloader)}')\n",
    "\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch in bert_dataloader_test:\n",
    "        input_ids,mask,indices,senses = batch\n",
    "        input_ids.to(device)\n",
    "        mask.to(device)\n",
    "        indices.to(device)\n",
    "        senses = torch.tensor(senses).view(-1)\n",
    "        senses.to(device)\n",
    "        outputs = model(input_ids,mask,indices).to(device)\n",
    "        _,predicted = torch.max(outputs.to(device),1)\n",
    "        total_correct += (predicted.to(device) == senses.to(device)).sum().item()\n",
    "        total += senses.size(0)\n",
    "    accuracy = total_correct/total\n",
    "    print(\"correct:\",total_correct)\n",
    "    print(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the difference between the two LSTMs that you have implemented for word sense disambiguation.\n",
    "\n",
    "Important note: your LSTMs should be nearly the same, but your linear layer must take different inputs. Describe why and how you think this difference will affect the performance of different LSTMs. How does the contextual representation of the whole sequence perform? How does the representation of the target word perform? What is better and for what situations? Why do we observe these differences?\n",
    "\n",
    "What kind of representations are the different approaches using to predict word senses?\n",
    "\n",
    "**[4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM MODEL 1 RESULTS: \n",
    "# ___________________________________\n",
    "# Epoch 1, Loss : 1.786867834724618\n",
    "# Epoch 2, Loss : 1.5498475718545255\n",
    "# Epoch 3, Loss : 1.540198791563942\n",
    "# Epoch 4, Loss : 1.5327540545460427\n",
    "# Epoch 5, Loss : 1.5270262987577625\n",
    "\n",
    "# correct: 214\n",
    "# accuracy: 0.11251314405888538 = ~ 11 %\n",
    "\n",
    "# *******************************************************\n",
    "\n",
    "# LSTM MODEL 2 RESULTS:\n",
    "# ____________________________________\n",
    "# Epoch 1, Loss : 4.660387977497269\n",
    "# Epoch 2, Loss : 4.159341004426192\n",
    "# Epoch 3, Loss : 3.5921727566480794\n",
    "# Epoch 4, Loss : 2.8516270564466146\n",
    "# Epoch 5, Loss : 2.022744782898945\n",
    "\n",
    "# correct: 76\n",
    "# accuracy: 0.03995793901156677 = ~ 4 %\n",
    "\n",
    "# ********************************************************\n",
    "\n",
    "# DISCUSSION :\n",
    "# ____________________________________\n",
    "# Both of our LSTM models perform quite poorly, reaching at best an accuracy of 11%. We were somewhat surprised of the low performance!\n",
    "# Nevertheless, the first model performs (quite significantly) better than the second one. Could the reason be that the overall context-representation that the second model uses can be too ambiguous / general?\n",
    "# Also, it might be that specifying the target word's position in the first model aids the model to perform better.\n",
    "\n",
    "# EXPLANATIONS : \n",
    "# _____________________________________\n",
    "# Model 1 captures the context around the target word by using the hidden state from the LSTM at the target word's position. The LSTM processes the entire sentence, and the hidden state at the target word's index captures its context-specific representation. This hidden state is then passed to a linear classifier to predict the word sense, emphasizing the immediate surrounding context of the word.\n",
    "\n",
    "# Model 2 captures the overall sentence context by using the final hidden state from the LSTM, which represents the entire input sequence. The LSTM processes the whole sentence, and the final hidden state reflects the global context. This final hidden state is then passed to a linear classifier to predict the word sense, focusing on the broader context of the entire sentence.\n",
    "\n",
    "# Difference:\n",
    "# Model 1 focuses on the hidden state at the target word's position, reflecting the immediate context around the target word. Model 2 uses the final hidden state of the LSTM, reflecting the context of the entire sentence.\n",
    "\n",
    "# Performance (implications):\n",
    "# Model 1: Better for tasks where the immediate context around the word is sufficient for disambiguation.\n",
    "# Model 2: Better for tasks requiring an understanding of the entire sentence to disambiguate the word sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model with per-word form *accuracy* and comment on the results you get. How does the model perform in comparison to the baseline, and how do the models compare to each other? \n",
    "\n",
    "Expand on the evaluation by sorting the word-forms by the number of senses they have. Are word forms with fewer senses easier to predict? Give a short explanation of the results you get based on the number of senses per word.\n",
    "\n",
    "**[4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately we were not able to calculate per-word form accuracies.\n",
    "# The amount of training data lead to heavy computations that we were forced to run on the server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the LSTMs perform in comparison to BERT? What's the difference between representations obtained by the LSTMs and BERT?\n",
    "\n",
    "**[4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   BERT MODEL RESULTS (RUN IN THE MLTSERVER ):\n",
    "# _________________________________________________\n",
    "# Epoch 1, Loss : 4.733702104300123\n",
    "# Epoch 2, Loss : 3.405372674335076\n",
    "# Epoch 3, Loss : 2.1753532283012995\n",
    "# Epoch 4, Loss : 1.5793246047227183\n",
    "# Epoch 5, Loss : 1.2809866848762375\n",
    "# correct: 5536\n",
    "# ACCURACY : 0.36397107166337933 = ~ 36 %\n",
    "\n",
    "# BERT's perfomance is significantly better than the two LSTM models'. This can be due to the fact that we are using a pre-trained model with pre-trained word embeddings.\n",
    "# Differences in representations compared to LSTMs: fine-tuning representations that already exist, unsequential way BERT deals with the contexts, the concept of attention.\n",
    "# We noticed that the BERT model performed better when more epochs are added and the learning rate is decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could we do to improve all WSD models that we have worked with in this assignment?\n",
    "\n",
    "**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -Use pre-trained word embeddings instead of creating them from scratch.\n",
    "# -Add more epochs.\n",
    "# -Explore using dropout and mask (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings\n",
    "\n",
    "[1] Kågebäck, M., & Salomonsson, H. (2016). Word Sense Disambiguation using a Bidirectional LSTM. arXiv preprint arXiv:1606.03568.\n",
    "\n",
    "[2] ON WSD: https://web.stanford.edu/~jurafsky/slp3/slides/Chapter18.wsd.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "This assignment has a total of 46 marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   We worked on this assignment for two weeks almost every day. Most of our calls / meetings were ~4-5 hours.\n",
    "#   Contributions were equal among the groupmembers, although Eleni Fysikoudi was the leading person of the group, doing a great deal of the troubleshooting and debugging.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
